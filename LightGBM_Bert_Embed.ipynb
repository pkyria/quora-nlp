{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc77680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c73d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"quora.csv\")\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df = df.drop(['id', 'qid1', 'qid2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9f904",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a2e02",
   "metadata": {},
   "source": [
    "Because we use contexual similarity models (BERT & Sentence-BERT embeddings), stopwords will not be removed, to preserve semantic and syntactic meaning. We can filter them out for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28b8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_word = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at', 'to', 'of', 'is', 'it'}\n",
    "\n",
    "# # Function to remove stopwords\n",
    "# def remove_stopwords(text):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     words = word_tokenize(text.lower())\n",
    "#     filtered_words = [word for word in words if word not in stop_word]\n",
    "#     return ' '.join(filtered_words)\n",
    "\n",
    "# # Apply to both questions\n",
    "# df['question1'] = df['question1'].apply(remove_stopwords)\n",
    "# df['question2'] = df['question2'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "#Contractions handling\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "df['question1'] = df['question1'].apply(expand_contractions)\n",
    "df['question2'] = df['question2'].apply(expand_contractions)\n",
    "\n",
    "\n",
    "# Clean punctuation and special characters\n",
    "def remove_special_characters(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['question1'] = df['question1'].apply(remove_special_characters)\n",
    "df['question2'] = df['question2'].apply(remove_special_characters)\n",
    "\n",
    "#Remove URLs and special Characters\n",
    "def remove_urls(text):\n",
    "    cleaned_text = re.sub(r'http\\S+', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['question1'] = df['question1'].apply(remove_urls)\n",
    "df['question2'] = df['question2'].apply(remove_urls)\n",
    "\n",
    "\n",
    "#Remove html tags func\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "    return cleaned_text\n",
    "\n",
    "df['question1'] = df['question1'].apply(remove_html_tags)\n",
    "df['question2'] = df['question2'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d746d1",
   "metadata": {},
   "source": [
    "### Common Words - Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9b1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = list(df.question1)\n",
    "q2 = list(df.question2)\n",
    "\n",
    "\n",
    "def common_words(q1, q2):\n",
    "    length = len(q1) + len(q2)\n",
    "    common = q1.intersection(q2)\n",
    "    common_norm = len(common) / length\n",
    "    return common_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450356ab",
   "metadata": {},
   "source": [
    "### Sentence-BERT (Sentence embeddings for contexual similarity)\n",
    "\n",
    "Pretrained Sentence-BERT model (all-MiniLM-L6-v2) from HuggingFace. Maps sentences to dense 384-dimensional vector embeddings that capture semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f612d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rapidfuzz.fuzz import ratio\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "transformer = \"all-mpnet-base-v2\"\n",
    "\n",
    "def compute_fuzz_features(q1, q2):\n",
    "    fuzz_scores = []\n",
    "    for q1, q2 in zip(q1, q2):\n",
    "        fuzz_scores.append([ratio(q1, q2) / 100])  # Normalize to [0, 1]\n",
    "    return torch.tensor(fuzz_scores, dtype=torch.float32)\n",
    "\n",
    "# def sentence_bert_model_training(q1, q2):\n",
    "#     '''BERT Sentence Transformer embedding'''\n",
    "#     model = SentenceTransformer(transformer)\n",
    "\n",
    "#     # Check if GPU is available and move model to GPU\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     # Encode all questions in batches\n",
    "#     q1_embeddings = model.encode(df['question1'].tolist(), convert_to_tensor=True, batch_size=92, show_progress_bar= True)\n",
    "#     q2_embeddings = model.encode(df['question2'].tolist(), convert_to_tensor=True, batch_size=92, show_progress_bar= True)\n",
    "\n",
    "#     q1_tokenized = [set(word_tokenize(q.lower())) for q in q1]\n",
    "#     q2_tokenized = [set(word_tokenize(q.lower())) for q in q2]\n",
    "\n",
    "#     features = []\n",
    "#     for emb1, emb2, q1_words, q2_words in zip(q1_embeddings, q2_embeddings, q1_tokenized, q2_tokenized):\n",
    "\n",
    "#         # Compute cosine similarity\n",
    "#         cosine_sim = util.cos_sim(emb1, emb2).item()  # float\n",
    "#         cosine_sim = torch.tensor([cosine_sim], device=emb1.device)\n",
    "\n",
    "#         # Create feature vector: [cosine_sim] + abs diff + elementwise product\n",
    "#         diff = torch.abs(emb1 - emb2)\n",
    "#         mult = emb1 * emb2\n",
    "\n",
    "#         # Adding common word count normalized\n",
    "#         common_word_count_norm = common_words(q1_words, q2_words)\n",
    "#         common_word_count_norm = torch.tensor([common_word_count_norm], device=emb1.device, dtype=torch.float32)\n",
    "\n",
    "#         # Length difference, normalized\n",
    "#         word_count_diff = abs(len(q1_words) - len(q2_words))\n",
    "#         max_word_count = max(len(q1_words), len(q2_words), 1)  # Avoiding division by zero by taking the greatest length instead\n",
    "#         length_diff = word_count_diff / max_word_count\n",
    "#         length_diff = torch.tensor([length_diff], device=emb1.device, dtype=torch.float32)\n",
    "\n",
    "#         # Combine all features into one vector\n",
    "#         feature_vector = torch.cat([cosine_sim, diff, mult, common_word_count_norm, length_diff])\n",
    "#         features.append(feature_vector)\n",
    "\n",
    "\n",
    "\n",
    "#     # Stacking features from list to 2D tensor\n",
    "#     features_tensor = torch.stack(features)\n",
    "\n",
    "#     # Fuzz ratio = for similarity/synonimity between pairs\n",
    "#     fuzz_tensor = compute_fuzz_features(q1, q2).to(device)\n",
    "#     feat = torch.cat([features_tensor, fuzz_tensor], dim=1)\n",
    "    \n",
    "#     return feat.cpu().numpy()\n",
    "\n",
    "\n",
    "def sentence_bert_model_training(q1, q2, transformer='all-mpnet-base-v2', batch_size=1000):\n",
    "    '''BERT Sentence Transformer embedding with batch processing'''\n",
    "    model = SentenceTransformer(transformer)\n",
    "\n",
    "    # Check if GPU is available and move model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize lists to collect features\n",
    "    all_features = []\n",
    "    n_samples = len(q1)\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "    print(f\"Processing {n_samples} samples in {n_batches} batches...\")\n",
    "    for i in tqdm(range(n_batches), desc=\"Feature Extraction Batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, n_samples)\n",
    "        q1_batch = q1[start_idx:end_idx]\n",
    "        q2_batch = q2[start_idx:end_idx]\n",
    "\n",
    "        # Encode questions in the current batch\n",
    "        q1_embeddings = model.encode(q1_batch, convert_to_tensor=True, batch_size=92, show_progress_bar=False)\n",
    "        q2_embeddings = model.encode(q2_batch, convert_to_tensor=True, batch_size=92, show_progress_bar=False)\n",
    "\n",
    "        # Tokenize questions for lexical features\n",
    "        q1_tokenized = [set(word_tokenize(q.lower())) for q in q1_batch]\n",
    "        q2_tokenized = [set(word_tokenize(q.lower())) for q in q2_batch]\n",
    "\n",
    "        batch_features = []\n",
    "        for emb1, emb2, q1_words, q2_words in zip(q1_embeddings, q2_embeddings, q1_tokenized, q2_tokenized):\n",
    "            # Compute cosine similarity\n",
    "            cosine_sim = util.cos_sim(emb1, emb2).item()\n",
    "            cosine_sim = torch.tensor([cosine_sim], device=emb1.device, dtype=torch.float32)\n",
    "\n",
    "            # Create feature vector: [cosine_sim] + abs diff + elementwise product\n",
    "            diff = torch.abs(emb1 - emb2)\n",
    "            mult = emb1 * emb2\n",
    "\n",
    "            # Compute common words count and normalize\n",
    "            common_count = common_words(q1_words, q2_words)\n",
    "            union_count = len(q1_words.union(q2_words))\n",
    "            common_word_count_norm = common_count / union_count if union_count > 0 else 0\n",
    "            common_word_count_norm = torch.tensor([common_word_count_norm], device=emb1.device, dtype=torch.float32)\n",
    "\n",
    "            # Compute difference of word count and normalize\n",
    "            word_count_diff = abs(len(q1_words) - len(q2_words))\n",
    "            max_word_count = max(len(q1_words), len(q2_words), 1)\n",
    "            length_diff = word_count_diff / max_word_count\n",
    "            length_diff = torch.tensor([length_diff], device=emb1.device, dtype=torch.float32)\n",
    "\n",
    "            # Combine features into one vector\n",
    "            feature_vector = torch.cat([cosine_sim, diff, mult, common_word_count_norm, length_diff])\n",
    "            batch_features.append(feature_vector)\n",
    "\n",
    "        # Stack batch features and compute fuzz features\n",
    "        features_tensor = torch.stack(batch_features)\n",
    "        fuzz_tensor = compute_fuzz_features(q1_batch, q2_batch).to(device)\n",
    "        batch_feat = torch.cat([features_tensor, fuzz_tensor], dim=1)\n",
    "\n",
    "        # Move to CPU and append to all_features\n",
    "        all_features.append(batch_feat.cpu())\n",
    "\n",
    "        # Clear GPU memory\n",
    "        del q1_embeddings, q2_embeddings, features_tensor, fuzz_tensor, batch_feat\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all batch features on CPU\n",
    "    feat = torch.cat(all_features, dim=0)\n",
    "    return feat.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0fbadc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 404287 samples in 405 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Batches: 100%|██████████| 405/405 [24:34<00:00,  3.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['is_duplicate'].values  # Labels (0 or 1)\n",
    "\n",
    "# Extract features for the entire dataset\n",
    "X = sentence_bert_model_training(q1, q2)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03686562370169114, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03686562370169114, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.03686562370169114, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.05823806190862759, max_depth=7, min_child_weight=3, n_estimators=50, subsample=0.8; total time=  28.5s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.05823806190862759, max_depth=7, min_child_weight=3, n_estimators=50, subsample=0.8; total time=  28.6s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.05823806190862759, max_depth=7, min_child_weight=3, n_estimators=50, subsample=0.8; total time=  28.7s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.038126384474070636, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=  51.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.038126384474070636, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=  51.5s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.038126384474070636, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=  51.7s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.09803826053364555, max_depth=5, min_child_weight=1, n_estimators=50, subsample=0.9; total time=  20.3s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.09803826053364555, max_depth=5, min_child_weight=1, n_estimators=50, subsample=0.9; total time=  20.4s\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.09803826053364555, max_depth=5, min_child_weight=1, n_estimators=50, subsample=0.9; total time=  20.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.13724202708881442, max_depth=9, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.13724202708881442, max_depth=9, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 2.3min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.13724202708881442, max_depth=9, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 2.3min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1319962286886272, max_depth=9, min_child_weight=1, n_estimators=100, subsample=0.9; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1319962286886272, max_depth=9, min_child_weight=1, n_estimators=100, subsample=0.9; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1319962286886272, max_depth=9, min_child_weight=1, n_estimators=100, subsample=0.9; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.20972021398116938, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.20972021398116938, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.9, learning_rate=0.20972021398116938, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2064890322064891, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2064890322064891, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2064890322064891, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.7; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.07002308519307848, max_depth=7, min_child_weight=5, n_estimators=100, subsample=0.8; total time=  49.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.07002308519307848, max_depth=7, min_child_weight=5, n_estimators=100, subsample=0.8; total time=  49.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.07002308519307848, max_depth=7, min_child_weight=5, n_estimators=100, subsample=0.8; total time=  48.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.13907240378591026, max_depth=9, min_child_weight=5, n_estimators=200, subsample=0.7; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.13907240378591026, max_depth=9, min_child_weight=5, n_estimators=200, subsample=0.7; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.13907240378591026, max_depth=9, min_child_weight=5, n_estimators=200, subsample=0.7; total time= 2.2min\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': np.float64(0.03686562370169114), 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Reduced dataset size for hyperparameter tuning to prevent crashes\n",
    "subset_size = 0.3  # Using 30% of the data for hyperparameter tuning\n",
    "X_train_subset, _, y_train_subset, _ = train_test_split(\n",
    "    X_train, y_train, train_size=subset_size, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "model = LGBMClassifier(objective='binary', metric='binary_logloss')\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'learning_rate': stats.uniform(0.01, 0.29),  # Range: [0.01, 0.3]\n",
    "    'num_leaves': [15, 31, 63, 127],  # Powers of 2 - 1, controlling tree complexity\n",
    "    'n_estimators': [50, 100, 200, 300],  # Number of trees\n",
    "    'max_depth': [3, 5, 7, 9, -1],  # -1 means no limit\n",
    "    'min_child_weight': [1e-3, 1e-2, 1e-1, 1, 3, 5],  # Small values for fine control\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],  # Fraction of samples\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],  # Fraction of features\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1.0],  # L1 regularization\n",
    "    'reg_lambda': [0, 0.01, 0.1, 1.0],  # L2 regularization\n",
    "    'min_child_samples': [10, 20, 30, 50]  # Minimum samples in a leaf\n",
    "}\n",
    "\n",
    "# Setup the randomized search\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='neg_log_loss', cv=3, verbose=1)\n",
    "\n",
    "# Fit the randomized search\n",
    "random_search.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Get the best parameters and model\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "best_model = LGBMClassifier(**best_params, eval_metric='logloss', use_label_encoder=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4d250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.30713916629328697\n",
      "\n",
      "\n",
      "Accuracy: 0.8607558930470701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89     51026\n",
      "           1       0.79      0.84      0.82     29832\n",
      "\n",
      "    accuracy                           0.86     80858\n",
      "   macro avg       0.85      0.86      0.85     80858\n",
      "weighted avg       0.86      0.86      0.86     80858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model.fit(X_train, y_train, verbose = 2)\n",
    "\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Predict probabilities for log loss calculation\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# Compute Log Loss\n",
    "loss = log_loss(y_test, y_pred_proba)\n",
    "print(\"Log Loss:\", loss)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "best_model.save_model(\"lgbm_model_01.json\")\n",
    "\n",
    "#  Load the saved model for future use\n",
    "# model_loaded = XGBClassifier()\n",
    "# model_loaded.load_model(\"xgboost_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22257bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
