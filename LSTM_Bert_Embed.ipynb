{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc77680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c73d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"quora.csv\")\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df = df.drop(['id', 'qid1', 'qid2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9f904",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a2e02",
   "metadata": {},
   "source": [
    "Because we use contexual similarity models (BERT & Sentence-BERT embeddings), stopwords will not be removed, to preserve semantic and syntactic meaning. We can filter them out for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a28b8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contractions handling\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "df['question1'] = df['question1'].apply(expand_contractions)\n",
    "df['question2'] = df['question2'].apply(expand_contractions)\n",
    "\n",
    "\n",
    "# Clean punctuation and special characters\n",
    "def remove_special_characters(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['question1'] = df['question1'].apply(remove_special_characters)\n",
    "df['question2'] = df['question2'].apply(remove_special_characters)\n",
    "\n",
    "#Remove URLs and special Characters\n",
    "def remove_urls(text):\n",
    "    cleaned_text = re.sub(r'http\\S+', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['question1'] = df['question1'].apply(remove_urls)\n",
    "df['question2'] = df['question2'].apply(remove_urls)\n",
    "\n",
    "\n",
    "#Remove html tags func\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "    return cleaned_text\n",
    "\n",
    "df['question1'] = df['question1'].apply(remove_html_tags)\n",
    "df['question2'] = df['question2'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d746d1",
   "metadata": {},
   "source": [
    "### Common Words - Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9b1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = list(df.question1)\n",
    "q2 = list(df.question2)\n",
    "\n",
    "\n",
    "def common_words(q1, q2):\n",
    "    length = len(q1) + len(q2)\n",
    "    common = q1.intersection(q2)\n",
    "    common_norm = len(common) / length\n",
    "    return common_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450356ab",
   "metadata": {},
   "source": [
    "### Sentence-BERT (Sentence embeddings for contexual similarity)\n",
    "\n",
    "Pretrained Sentence-BERT model (all-MiniLM-L6-v2) from HuggingFace. Maps sentences to dense 384-dimensional vector embeddings that capture semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f612d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rapidfuzz.fuzz import ratio\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "transformer = \"all-mpnet-base-v2\"\n",
    "\n",
    "def compute_fuzz_features(q1, q2):\n",
    "    fuzz_scores = []\n",
    "    for q1, q2 in zip(q1, q2):\n",
    "        fuzz_scores.append([ratio(q1, q2) / 100])  # Normalize to [0, 1]\n",
    "    return torch.tensor(fuzz_scores, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def sentence_bert_model_training(q1, q2, transformer='all-mpnet-base-v2', batch_size=1000):\n",
    "    '''BERT Sentence Transformer embedding with batch processing'''\n",
    "    model = SentenceTransformer(transformer)\n",
    "\n",
    "    # Check if GPU is available and move model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize lists to collect features\n",
    "    all_embeddings_q1 = []\n",
    "    all_embeddings_q2 = []\n",
    "    all_features = []\n",
    "    n_samples = len(q1)\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "    print(f\"Processing {n_samples} samples in {n_batches} batches...\")\n",
    "    for i in tqdm(range(n_batches), desc=\"Feature Extraction Batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, n_samples)\n",
    "        q1_batch = q1[start_idx:end_idx]\n",
    "        q2_batch = q2[start_idx:end_idx]\n",
    "\n",
    "        # Encode questions in the current batch\n",
    "        q1_embeddings = model.encode(q1_batch, convert_to_tensor=True, batch_size=92, show_progress_bar=False)\n",
    "        q2_embeddings = model.encode(q2_batch, convert_to_tensor=True, batch_size=92, show_progress_bar=False)\n",
    "\n",
    "        # Tokenize questions for lexical features\n",
    "        q1_tokenized = [set(word_tokenize(q.lower())) for q in q1_batch]\n",
    "        q2_tokenized = [set(word_tokenize(q.lower())) for q in q2_batch]\n",
    "\n",
    "        batch_features = []\n",
    "        for emb1, emb2, q1_words, q2_words in zip(q1_embeddings, q2_embeddings, q1_tokenized, q2_tokenized):\n",
    "            # Compute cosine similarity\n",
    "            cosine_sim = util.cos_sim(emb1, emb2).item()\n",
    "            cosine_sim = torch.tensor([cosine_sim], device=emb1.device, dtype=torch.float32) # Dimensionality = 1\n",
    "\n",
    "            # Create feature vector: [cosine_sim] + abs diff + elementwise product\n",
    "            diff = torch.abs(emb1 - emb2) # Dimensionality = 384 (it is a tensor with the same dim with the embedding)\n",
    "            mult = emb1 * emb2\n",
    "\n",
    "            # Compute common words count and normalize\n",
    "            common_count = common_words(q1_words, q2_words)\n",
    "            union_count = len(q1_words.union(q2_words))\n",
    "            common_word_count_norm = common_count / union_count if union_count > 0 else 0\n",
    "            common_word_count_norm = torch.tensor([common_word_count_norm], device=emb1.device, dtype=torch.float32) # Dimensionality = 384 (it is a tensor with the same dim with the embedding)\n",
    "\n",
    "            # Compute difference of word count and normalize\n",
    "            word_count_diff = abs(len(q1_words) - len(q2_words))\n",
    "            max_word_count = max(len(q1_words), len(q2_words), 1)\n",
    "            length_diff = word_count_diff / max_word_count\n",
    "            length_diff = torch.tensor([length_diff], device=emb1.device, dtype=torch.float32) # Dimensionality 1\n",
    "\n",
    "            # Combine features into one vector\n",
    "            feature_vector = torch.cat([cosine_sim, diff, mult, common_word_count_norm, length_diff]) # Dimensionality = 1 + 384 + 384 + 1 = 771\n",
    "            batch_features.append(feature_vector)\n",
    "\n",
    "        # Stack batch features and compute fuzz features\n",
    "        features_tensor = torch.stack(batch_features)\n",
    "        fuzz_tensor = compute_fuzz_features(q1_batch, q2_batch).to(device) # Dimensionality 1\n",
    "        batch_feat = torch.cat([features_tensor, fuzz_tensor], dim=1) # Dimensionality = 1 + 384 + 384 + 1 + 1 = 772\n",
    "\n",
    "        # Store embeddings and features\n",
    "        all_embeddings_q1.append(q1_embeddings.cpu())\n",
    "        all_embeddings_q2.append(q2_embeddings.cpu())\n",
    "        all_features.append(batch_feat.cpu())\n",
    "\n",
    "        # Clear GPU memory\n",
    "        del q1_embeddings, q2_embeddings, features_tensor, fuzz_tensor, batch_feat\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all batch results\n",
    "    embeddings_q1 = torch.cat(all_embeddings_q1, dim=0)\n",
    "    embeddings_q2 = torch.cat(all_embeddings_q2, dim=0)\n",
    "    features = torch.cat(all_features, dim=0)\n",
    "    \n",
    "    # We pass all embeddings and feats as tensors, not numpy arrays like the ML models, to later feed to our LSTM layers\n",
    "    return embeddings_q1, embeddings_q2, features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22257bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentencePairLSTMDataset(Dataset):\n",
    "    def __init__(self, embeddings_q1, embeddings_q2, features, labels):\n",
    "        self.embeddings_q1 = embeddings_q1  # tensor with shape (n_samples, embedding_dim)\n",
    "        self.embeddings_q2 = embeddings_q2  # tensor with shape (n_samples, embedding_dim)\n",
    "        self.features = features  # tensor with shape (n_samples, feature_dim)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)  # tensor with shape (n_samples,)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'emb_q1': self.embeddings_q1[idx],\n",
    "            'emb_q2': self.embeddings_q2[idx],\n",
    "            'features': self.features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0354391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim = 384, feature_dim = 772, lstm_hidden_dim=128, dropout=0.3): # Embedding dim for MiniLM transformer = 384. Feat dim = 772\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # Input dimension: 2 * embedding_dim (q1 + q2) + feature_dim\n",
    "        input_dim = 2 * embedding_dim + feature_dim\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, emb_q1, emb_q2, features):\n",
    "        # Concatenate inputs along the feature dimension to combine all information.\n",
    "        # torch.cat concatenates tensors along dim=1 (the feature/embedding dimension).\n",
    "        combined = torch.cat([emb_q1, emb_q2, features], dim=1) # shape: (batch_size, embedding_dim + embedding_dim + feature_dim).\n",
    "        \n",
    "        # LSTM expects input shape: (batch_size, seq_len, input_dim).\n",
    "        # currently is (batch_size, input_dim). Add a sequence dimension (seq_len=1).\n",
    "        combined = combined.unsqueeze(1) # unsqueeze(1) adds a dimension at index 1, making shape: (batch_size, 1, input_dim).\n",
    "        \n",
    "        # Pass the combined tensor through the LSTM layer.\n",
    "        # self.lstm is defined in __init__ as nn.LSTM(input_size=input_dim, hidden_size=lstm_hidden_dim)\n",
    "        # Input shape: (batch_size, seq_len, input_dim)\n",
    "        # Output: lstm_out is (batch_size, seq_len, lstm_hidden_dim)\n",
    "        # _ contains hidden states (h_n, c_n), which we ignore since we only need the output\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        \n",
    "        # Extract the LSTM output for the last time step (seq_len=1, so only one step exists).\n",
    "        # Indexing [:, -1, :] selects the last step: (batch_size, lstm_hidden_dim).\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply dropout for regularization to prevent overfitting.\n",
    "        # self.dropout is defined in __init__ as nn.Dropout(dropout_rate), e.g., 0.3.\n",
    "        # Input/output shape remains: (batch_size, lstm_hidden_dim), e.g., (32, 128).\n",
    "        # Randomly zeros out elements with probability dropout_rate during training.\n",
    "        out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Pass through the fully connected (linear) layer for classification.\n",
    "        # self.fc is defined in __init__ as nn.Linear(lstm_hidden_dim, 1).\n",
    "        # Input shape: (batch_size, lstm_hidden_dim), e.g., (32, 128).\n",
    "        # Output shape: (batch_size, 1), e.g., (32, 1), representing raw scores (logits).\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Apply sigmoid activation to convert logits to probabilities (0 to 1).\n",
    "        # self.sigmoid is defined in __init__ as nn.Sigmoid().\n",
    "        # Input shape: (batch_size, 1). Output shape: (batch_size, 1).\n",
    "        # squeeze() removes dimensions of size 1, making shape: (batch_size,).\n",
    "        # Example: (32, 1) -> (32,). This matches the expected shape for BCELoss.\n",
    "        out = self.sigmoid(out).squeeze()\n",
    "        \n",
    "        # Return the final output: predicted probabilities for each sample in the batch.\n",
    "        # Shape: (batch_size,), e.g., (32,). Values in [0, 1] for binary classification.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eaa8ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 323429 samples in 324 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Batches: 100%|██████████| 324/324 [19:53<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 80858 samples in 81 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Batches: 100%|██████████| 81/81 [05:01<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Train Loss (Log Loss): 0.3183\n",
      "Val Loss (Log Loss): 0.2977, Val Accuracy: 0.8650\n",
      "Epoch 2/15\n",
      "Train Loss (Log Loss): 0.2840\n",
      "Val Loss (Log Loss): 0.2814, Val Accuracy: 0.8737\n",
      "Epoch 3/15\n",
      "Train Loss (Log Loss): 0.2634\n",
      "Val Loss (Log Loss): 0.2702, Val Accuracy: 0.8800\n",
      "Epoch 4/15\n",
      "Train Loss (Log Loss): 0.2461\n",
      "Val Loss (Log Loss): 0.2666, Val Accuracy: 0.8826\n",
      "Epoch 5/15\n",
      "Train Loss (Log Loss): 0.2291\n",
      "Val Loss (Log Loss): 0.2613, Val Accuracy: 0.8858\n",
      "Epoch 6/15\n",
      "Train Loss (Log Loss): 0.2136\n",
      "Val Loss (Log Loss): 0.2638, Val Accuracy: 0.8864\n",
      "Epoch 7/15\n",
      "Train Loss (Log Loss): 0.1977\n",
      "Val Loss (Log Loss): 0.2615, Val Accuracy: 0.8879\n",
      "Epoch 8/15\n",
      "Train Loss (Log Loss): 0.1836\n",
      "Val Loss (Log Loss): 0.2621, Val Accuracy: 0.8896\n",
      "Epoch 9/15\n",
      "Train Loss (Log Loss): 0.1703\n",
      "Val Loss (Log Loss): 0.2708, Val Accuracy: 0.8898\n",
      "Epoch 10/15\n",
      "Train Loss (Log Loss): 0.1580\n",
      "Val Loss (Log Loss): 0.2724, Val Accuracy: 0.8905\n",
      "Epoch 11/15\n",
      "Train Loss (Log Loss): 0.1457\n",
      "Val Loss (Log Loss): 0.2805, Val Accuracy: 0.8904\n",
      "Epoch 12/15\n",
      "Train Loss (Log Loss): 0.1356\n",
      "Val Loss (Log Loss): 0.2877, Val Accuracy: 0.8888\n",
      "Epoch 13/15\n",
      "Train Loss (Log Loss): 0.1263\n",
      "Val Loss (Log Loss): 0.2915, Val Accuracy: 0.8916\n",
      "Epoch 14/15\n",
      "Train Loss (Log Loss): 0.1177\n",
      "Val Loss (Log Loss): 0.3012, Val Accuracy: 0.8910\n",
      "Epoch 15/15\n",
      "Train Loss (Log Loss): 0.1102\n",
      "Val Loss (Log Loss): 0.3099, Val Accuracy: 0.8920\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "q1 = df['question1'].values\n",
    "q2 = df['question2'].values\n",
    "labels = df['is_duplicate'].values\n",
    "\n",
    "# Split data\n",
    "q1_train, q1_val, q2_train, q2_val, labels_train, labels_val = train_test_split(\n",
    "    q1, q2, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Get embeddings and features\n",
    "emb_q1_train, emb_q2_train, features_train = sentence_bert_model_training(q1_train, q2_train)\n",
    "emb_q1_val, emb_q2_val, features_val = sentence_bert_model_training(q1_val, q2_val)\n",
    "\n",
    "# Creating datasets as tensors\n",
    "train_dataset = SentencePairLSTMDataset(emb_q1_train, emb_q2_train, features_train, labels_train)\n",
    "val_dataset = SentencePairLSTMDataset(emb_q1_val, emb_q2_val, features_val, labels_val)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Initializing model\n",
    "embedding_dim = emb_q1_train.size(1)  \n",
    "feature_dim = features_train.size(1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMClassifier(embedding_dim, feature_dim).to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Binary Crossentropy Loss is the Log Loss we are looking to monitor\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        emb_q1 = batch['emb_q1'].to(device)\n",
    "        emb_q2 = batch['emb_q2'].to(device)\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(emb_q1, emb_q2, features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            emb_q1 = batch['emb_q1'].to(device)\n",
    "            emb_q2 = batch['emb_q2'].to(device)\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(emb_q1, emb_q2, features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "# Train the model\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    print(f'Train Loss (Log Loss): {train_loss:.4f}')\n",
    "    print(f'Val Loss (Log Loss): {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
